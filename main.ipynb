{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP with python\r\n",
    "\r\n",
    "Dataset: Reuters-21578, Distribution 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import reuters, stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.pipeline import make_pipeline\r\n",
    "from collections import Counter\r\n",
    "from collections import OrderedDict\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data\r\n",
    "\r\n",
    "The distribution of categories in the corpus is highly skewed, with 36.7% of the documents in the most common category, and only 0.0185% (2 documents) in each of the five least common categories so let's use most frequent ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding most common categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final categories: ['earn', 'acq', 'money-fx', 'grain', 'crude', 'trade', 'interest', 'wheat', 'ship', 'corn', 'money-supply', 'dlr', 'sugar', 'oilseed', 'coffee', 'gnp', 'gold', 'veg-oil', 'soybean', 'nat-gas', 'livestock', 'bop', 'cpi', 'cocoa', 'reserves', 'carcass', 'copper', 'jobs', 'yen', 'ipi']\n"
     ]
    }
   ],
   "source": [
    "training_ids = [doc_id for doc_id in reuters.fileids() if doc_id.startswith('training')]\r\n",
    "\r\n",
    "cat_counts = dict()\r\n",
    "\r\n",
    "for id in training_ids:\r\n",
    "    for i in range(len(reuters.categories(id))):\r\n",
    "        category = reuters.categories(id)[i]\r\n",
    "        cat_counts[category] = cat_counts.get(category, 0) + 1\r\n",
    "\r\n",
    "num_categories = 30     #select documents for 30 most common categories\r\n",
    "\r\n",
    "cat_sorted = sorted(cat_counts.items(), key=lambda x: x[1], reverse=True)[:num_categories]\r\n",
    "categories = [item[0] for item in cat_sorted]\r\n",
    "\r\n",
    "print(\"Final categories:\", categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_ids = [id for id in training_ids if reuters.categories(id)[0] in categories]\r\n",
    "test_ids = [id for id in reuters.fileids() if id.startswith('test') and reuters.categories(id)[0] in categories]\r\n",
    "categories = {category: id for id, category in enumerate(reuters.categories())}\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\r\n",
    "    \"\"\"\r\n",
    "    A function that normalizes given text in string format\r\n",
    "    \"\"\"\r\n",
    "    punctuations = r'''.,!?;\\\"\\'-+(){}[]\"\\,<>./?@$&''',\r\n",
    "    stop_words = stopwords.words('english')\r\n",
    "\r\n",
    "    # Removing the punctuations\r\n",
    "    for x in string.lower(): \r\n",
    "        if x in punctuations: \r\n",
    "            string = string.replace(x, \"\") \r\n",
    "\r\n",
    "    # Converting the text to lower\r\n",
    "    string = string.lower()\r\n",
    "\r\n",
    "    # Removing stop words\r\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\r\n",
    "\r\n",
    "    # Cleaning the whitespaces\r\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\r\n",
    "\r\n",
    "    return string  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\r\n",
    "    \"\"\"\r\n",
    "    A function that normalizes and divides given text\r\n",
    "    into words.\r\n",
    "    \"\"\"\r\n",
    "    re_pattern = re.compile(r'([\\s\\d.,!?;\\\"\\'-+])+')\r\n",
    "    stop_words = stopwords.words('english')\r\n",
    "\r\n",
    "    doc = clean_text(doc)\r\n",
    "\r\n",
    "    tokens = re_pattern.split(doc.lower())\r\n",
    "    tokens = [token for token in tokens if token not in ',.!?\\'\\\"-\\t\\n ;']\r\n",
    "    tokens = [token for token in tokens if token not in stop_words]\r\n",
    "\r\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\r\n",
    "tokens = []\r\n",
    "DF = {}\r\n",
    "term_frequency = [] #frequency of a given word\r\n",
    "ids = [] #\r\n",
    "doc_lengths = [] #number of words in document\r\n",
    "\r\n",
    "for i, id in enumerate(training_ids):\r\n",
    "    document = reuters.raw(id)\r\n",
    "\r\n",
    "    vocab = tokenize(document)\r\n",
    "    tokens += [vocab]\r\n",
    "\r\n",
    "    for w in vocab:\r\n",
    "        try:\r\n",
    "            DF[w].add(i)\r\n",
    "        except:\r\n",
    "            DF[w] = {i}\r\n",
    "\r\n",
    "    corpus[id] = dict((tok, 1) for tok in vocab)\r\n",
    "    ids.append(id)\r\n",
    "    doc_lengths.append(len(vocab))\r\n",
    "\r\n",
    "for i in DF:\r\n",
    "    DF[i] = len(DF[i])\r\n",
    "\r\n",
    "for token in vocab:\r\n",
    "    term_frequency.append(DF[token])\r\n",
    "\r\n",
    "stop_words = nltk.corpus.stopwords.words('english')\r\n",
    "words = reuters.words()\r\n",
    "words = [token.lower() for token in words if token not in ',&><.!/?\\'\\\"-\\t\\n ;']\r\n",
    "words = [token for token in words if token not in stop_words]\r\n",
    "\r\n",
    "N = len(words)\r\n",
    "token_counts = Counter(words)\r\n",
    "\r\n",
    "lexicon = sorted(set(sum(tokens, [])))# initialize lexicon of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for id in training_ids:\n",
    "    train_data.append(clean_text(reuters.raw(id)))\n",
    "\n",
    "for id in test_ids:\n",
    "    test_data.append(clean_text(reuters.raw(id)))\n",
    "\n",
    "train_target = [categories[reuters.categories(id)[0]] for id in training_ids]\n",
    "test_target = [categories[reuters.categories(id)[0]] for id in test_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of text tokenization methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original  | My Method  | NLTK Tokenizer\n",
      "-----------------------------------------\n",
      "    GREECE  |      greece  |      GREECE\n",
      "      BUYS  |        buys  |        BUYS\n",
      "    55,000  |      tonnes  |      55,000\n",
      "    TONNES  |      french  |      TONNES\n",
      "    FRENCH  |       maize  |      FRENCH\n",
      "     MAIZE  |       trade  |       MAIZE\n",
      "         -  |      greece  |           -\n",
      "     TRADE  |      bought  |       TRADE\n",
      "    Greece  |       total  |      Greece\n",
      "    bought  |      tonnes  |      bought\n",
      "         a  |      french  |           a\n",
      "     total  |       maize  |       total\n",
      "        of  |    tendered  |          of\n",
      "    55,000  |   yesterday  |      55,000\n",
      "    tonnes  |   initially  |      tonnes\n",
      "        of  |      tonnes  |          of\n",
      "    French  |       april  |      French\n",
      "     maize  |    delivery  |       maize\n",
      "      when  |       trade  |        when\n",
      "        it  |     sources  |          it\n"
     ]
    }
   ],
   "source": [
    "sample_text = reuters.raw('training/10489')\r\n",
    "oryginal = sample_text.split()\r\n",
    "my_method = tokenize(sample_text)\r\n",
    "nltk_tokenizer = word_tokenize(sample_text)\r\n",
    "\r\n",
    "print('  Original  | My Method  | NLTK Tokenizer')\r\n",
    "print('-' * 41)\r\n",
    "for original, my, nltk in zip(oryginal[:20], my_method[:20], nltk_tokenizer[:20]):\r\n",
    "    print(str(original).rjust(10, ' '), ' | ', str(my).rjust(10, ' '), ' | ', str(nltk).rjust(10, ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming & Lemmanization\r\n",
    "\r\n",
    "Stemming is a process of merging and converting words into groups that are derived from the same stem. It simply gets rid of the last letters of the word to obtain a shorter form. \r\n",
    "\r\n",
    "Lemmatization is a process of converting the given word into its base form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Original  |    Stemmer   |   Lemmatizer\n",
      "-----------------------------------------\n",
      "    greece  |       greec  |      greece\n",
      "      buys  |         buy  |         buy\n",
      "    tonnes  |        tonn  |       tonne\n",
      "    french  |      french  |      french\n",
      "     maize  |        maiz  |       maize\n",
      "     trade  |       trade  |       trade\n",
      "    greece  |       greec  |      greece\n",
      "    bought  |      bought  |      bought\n",
      "     total  |       total  |       total\n",
      "    tonnes  |        tonn  |       tonne\n",
      "    french  |      french  |      french\n",
      "     maize  |        maiz  |       maize\n",
      "  tendered  |      tender  |    tendered\n",
      " yesterday  |   yesterday  |   yesterday\n",
      " initially  |       initi  |   initially\n",
      "    tonnes  |        tonn  |       tonne\n",
      "     april  |       april  |       april\n",
      "  delivery  |    deliveri  |    delivery\n",
      "     trade  |       trade  |       trade\n",
      "   sources  |       sourc  |      source\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.stem.porter import PorterStemmer\r\n",
    "\r\n",
    "wnl = WordNetLemmatizer()\r\n",
    "\r\n",
    "stemmer = PorterStemmer()\r\n",
    "stemmed = [stemmer.stem(token) for token in my_method]\r\n",
    "lemm = [wnl.lemmatize(token) for token in my_method]\r\n",
    "\r\n",
    "print('  Original  |    Stemmer   |   Lemmatizer')\r\n",
    "print('-' * 41)\r\n",
    "for original, stem, lem in zip(my_method[:20], stemmed[:20], lemm[:20]):\r\n",
    "    print(str(original).rjust(10, ' '), ' | ', str(stem).rjust(10, ' '), ' | ', str(lem).rjust(10, ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF vectors\n",
    "\n",
    "TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15221529457097957"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = {}\r\n",
    "\r\n",
    "for doc in corpus:\r\n",
    "    for token in corpus[doc]:\r\n",
    "        tf = corpus[doc][token] / len(corpus[doc])\r\n",
    "        df = token_counts[token] / N\r\n",
    "        idf = np.log(N / (df + 1))  #add 1 to avoid possibility of dividing by zero if there's no instance of the word in lexicon\r\n",
    "        tf_idf[doc, token] = tf*idf\r\n",
    "\r\n",
    "tf_idf['training/10', 'computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(k, query):\n",
    "    preprocessed_query = tokenize(query)\n",
    "    tokens = tokenize(str(preprocessed_query))\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    l = []\n",
    "    for i in query_weights[:k]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training/12500', 'training/7215', 'training/9304', 'training/2169', 'training/12583', 'training/2183', 'training/4278', 'training/1396', 'training/6898', 'training/1057']\n",
      "['barley', 'corn', 'grain', 'wheat']\n",
      "['corn', 'grain', 'wheat']\n"
     ]
    }
   ],
   "source": [
    "#Show documents most simylar to the given one\n",
    "print(matching_score(10, reuters.raw('training/9865')))\n",
    "print(reuters.categories('training/9865'))\n",
    "print(reuters.categories('training/12500'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model with naive bayes\r\n",
    "model_tfidf = make_pipeline(TfidfVectorizer(), MultinomialNB())\r\n",
    "model_count = make_pipeline(CountVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test):\r\n",
    "    \r\n",
    "    model.fit(X_train, y_train)\r\n",
    "    \r\n",
    "    print(\"Accuracy on training set:\")\r\n",
    "    print(model.score(X_train, y_train))\r\n",
    "    print(\"Accuracy on testing set:\")\r\n",
    "    print(model.score(X_test, y_test))\r\n",
    "    \r\n",
    "    y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAccuracy of tfidf model\n",
      "Accuracy on training set:\n",
      "0.7247681396617567\n",
      "Accuracy on testing set:\n",
      "0.7135212272565109\n",
      "\tAccuracy of count based model\n",
      "Accuracy on training set:\n",
      "0.9253955264593563\n",
      "Accuracy on testing set:\n",
      "0.8683553335711738\n"
     ]
    }
   ],
   "source": [
    "print(\"\\tAccuracy of tfidf model\")\r\n",
    "train_and_evaluate(model_tfidf, train_data, test_data, train_target, test_target)\r\n",
    "print(\"\\tAccuracy of count based model\")\r\n",
    "train_and_evaluate(model_count, train_data, test_data, train_target, test_target)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "705918e780edd3c3d13d79c4436a820177c92fc972bec27704620ade7baffbb8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}